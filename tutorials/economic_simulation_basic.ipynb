{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Foundation!\n",
    "\n",
    "Foundation is the name of the economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). This is the first of several tutorials designed to explain how Foundation works and how it can be used to build simulation environments for studying economic problems.\n",
    "\n",
    "Just to orient you a bit, Foundation is specially designed for modeling economies in spatial, 2D grid worlds. The AI Economist paper uses a scenario with 4 agents in a world with *Stone* and *Wood*, which can be *collected*, *traded*, and used to build *Houses*. Here's a (nicely rendered) example of what such an environment looks like:\n",
    "\n",
    "![Foundation snapshot](assets/foundation_snapshot_rendered.jpg)\n",
    "\n",
    "This image just shows what you might see spatially. Behind the scenes, agents have inventories of Stone, Wood, and *Coin*, which they can exchange through a commodities marketplace. In addition, they periodically pay taxes on income earned through trading and building.\n",
    "\n",
    "**We've open-sourced Foundation to foster transparency and to enable others to build on it!** With that goal in mind, this first tutorial should give you enough to see how to create the type of simulation environment described above and how to interact with it. If you're interested to learn how it all works and how to build on it, make sure to check out the advanced tutorial as well! If, after that, you want to understand more about designing the simulation around economic problems, check out our tutorial explaining how the AI Economist uses Foundation to study the optimal taxation problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this **basic** tutorial, we will demonstrate the basics of how to create an instance of a simulation environment and how to interact with it.\n",
    "\n",
    "We will cover the following: \n",
    "1. Markov Decision Processes\n",
    "2. Creating a Simulation Environment (a Scenario Instance)\n",
    "3. Interacting with the Simulation\n",
    "4. Sampling and Visualizing an Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ai-economist\n",
      "  Using cached ai_economist-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting appdirs==1.4.4 (from ai-economist)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting appnope==0.1.2 (from ai-economist)\n",
      "  Using cached appnope-0.1.2-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting argon2-cffi==20.1.0 (from ai-economist)\n",
      "  Using cached argon2_cffi-20.1.0-cp37-abi3-macosx_10_6_intel.whl.metadata (7.9 kB)\n",
      "Collecting astroid==2.5.6 (from ai-economist)\n",
      "  Using cached astroid-2.5.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting async-generator==1.10 (from ai-economist)\n",
      "  Using cached async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting attrs==21.2.0 (from ai-economist)\n",
      "  Using cached attrs-21.2.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting backcall==0.2.0 (from ai-economist)\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting beautifulsoup4==4.9.3 (from ai-economist)\n",
      "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting black==21.5b1 (from ai-economist)\n",
      "  Using cached black-21.5b1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting bleach==3.3.0 (from ai-economist)\n",
      "  Using cached bleach-3.3.0-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Collecting bs4==0.0.1 (from ai-economist)\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Collecting certifi==2020.12.5 (from ai-economist)\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cffi==1.14.5 (from ai-economist)\n",
      "  Using cached cffi-1.14.5.tar.gz (475 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chardet==4.0.0 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (4.0.0)\n",
      "Collecting click==8.0.1 (from ai-economist)\n",
      "  Using cached click-8.0.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: cycler==0.10.0 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (0.10.0)\n",
      "Collecting decorator==5.0.9 (from ai-economist)\n",
      "  Using cached decorator-5.0.9-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting defusedxml==0.7.1 (from ai-economist)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting entrypoints==0.3 (from ai-economist)\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (1.1.0)\n",
      "Collecting flake8==3.9.2 (from ai-economist)\n",
      "  Using cached flake8-3.9.2-py2.py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting GPUtil==1.4.0 (from ai-economist)\n",
      "  Using cached GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: idna==2.10 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (2.10)\n",
      "Collecting iniconfig==1.1.1 (from ai-economist)\n",
      "  Using cached iniconfig-1.1.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ipykernel==5.5.5 (from ai-economist)\n",
      "  Using cached ipykernel-5.5.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting ipython==7.31.1 (from ai-economist)\n",
      "  Using cached ipython-7.31.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting ipython-genutils==0.2.0 (from ai-economist)\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting ipywidgets==7.6.3 (from ai-economist)\n",
      "  Using cached ipywidgets-7.6.3-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting isort==5.8.0 (from ai-economist)\n",
      "  Using cached isort-5.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jedi==0.18.0 (from ai-economist)\n",
      "  Using cached jedi-0.18.0-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting Jinja2==3.0.1 (from ai-economist)\n",
      "  Using cached Jinja2-3.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jsonschema==3.2.0 (from ai-economist)\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jupyter==1.0.0 (from ai-economist)\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
      "Collecting jupyter-client==6.1.12 (from ai-economist)\n",
      "  Using cached jupyter_client-6.1.12-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting jupyter-console==6.4.0 (from ai-economist)\n",
      "  Using cached jupyter_console-6.4.0-py3-none-any.whl.metadata (951 bytes)\n",
      "Collecting jupyter-core==4.7.1 (from ai-economist)\n",
      "  Using cached jupyter_core-4.7.1-py3-none-any.whl.metadata (759 bytes)\n",
      "Collecting jupyterlab-pygments==0.1.2 (from ai-economist)\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl.metadata (329 bytes)\n",
      "Collecting jupyterlab-widgets==1.0.0 (from ai-economist)\n",
      "  Using cached jupyterlab_widgets-1.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting kiwisolver==1.3.1 (from ai-economist)\n",
      "  Using cached kiwisolver-1.3.1.tar.gz (53 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lazy-object-proxy==1.6.0 (from ai-economist)\n",
      "  Using cached lazy-object-proxy-1.6.0.tar.gz (44 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lz4==3.1.3 (from ai-economist)\n",
      "  Using cached lz4-3.1.3.tar.gz (159 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting MarkupSafe==2.0.1 (from ai-economist)\n",
      "  Using cached MarkupSafe-2.0.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib==3.2.1 (from ai-economist)\n",
      "  Using cached matplotlib-3.2.1.tar.gz (40.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib-inline==0.1.2 (from ai-economist)\n",
      "  Using cached matplotlib_inline-0.1.2-py3-none-any.whl.metadata (380 bytes)\n",
      "Collecting mccabe==0.6.1 (from ai-economist)\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistune==0.8.4 (from ai-economist)\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting mypy-extensions==0.4.3 (from ai-economist)\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting nbclient==0.5.3 (from ai-economist)\n",
      "  Using cached nbclient-0.5.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting nbconvert==6.0.7 (from ai-economist)\n",
      "  Using cached nbconvert-6.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting nbformat==5.1.3 (from ai-economist)\n",
      "  Using cached nbformat-5.1.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting nest-asyncio==1.5.1 (from ai-economist)\n",
      "  Using cached nest_asyncio-1.5.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting notebook==6.4.1 (from ai-economist)\n",
      "  Using cached notebook-6.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy==1.21.0 (from ai-economist)\n",
      "  Using cached numpy-1.21.0.zip (10.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openpyxl==3.0.7 (from ai-economist)\n",
      "  Using cached openpyxl-3.0.7-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting packaging==20.9 (from ai-economist)\n",
      "  Using cached packaging-20.9-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pandas==1.2.4 (from ai-economist)\n",
      "  Using cached pandas-1.2.4.tar.gz (5.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandocfilters==1.4.3 (from ai-economist)\n",
      "  Using cached pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting parso==0.8.2 (from ai-economist)\n",
      "  Using cached parso-0.8.2-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pathspec==0.8.1 (from ai-economist)\n",
      "  Using cached pathspec-0.8.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (0.7.5)\n",
      "Collecting Pillow==9.0.0 (from ai-economist)\n",
      "  Using cached Pillow-9.0.0.tar.gz (49.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pluggy==0.13.1 (from ai-economist)\n",
      "  Using cached pluggy-0.13.1-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting prometheus-client==0.10.1 (from ai-economist)\n",
      "  Using cached prometheus_client-0.10.1-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting prompt-toolkit==3.0.18 (from ai-economist)\n",
      "  Using cached prompt_toolkit-3.0.18-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages (from ai-economist) (0.7.0)\n",
      "Collecting py==1.10.0 (from ai-economist)\n",
      "  Using cached py-1.10.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pycodestyle==2.7.0 (from ai-economist)\n",
      "  Using cached pycodestyle-2.7.0-py2.py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pycparser==2.20 (from ai-economist)\n",
      "  Using cached pycparser-2.20-py2.py3-none-any.whl.metadata (907 bytes)\n",
      "Collecting pycryptodome==3.10.1 (from ai-economist)\n",
      "  Using cached pycryptodome-3.10.1-cp35-abi3-macosx_10_9_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting pyflakes==2.3.1 (from ai-economist)\n",
      "  Using cached pyflakes-2.3.1-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Pygments==2.9.0 (from ai-economist)\n",
      "  Using cached Pygments-2.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pylint==2.8.2 (from ai-economist)\n",
      "  Using cached pylint-2.8.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pyparsing==2.4.7 (from ai-economist)\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pyrsistent==0.17.3 (from ai-economist)\n",
      "  Using cached pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytest==6.2.4 (from ai-economist)\n",
      "  Using cached pytest-6.2.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting python-dateutil==2.8.1 (from ai-economist)\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pytz==2021.1 (from ai-economist)\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pyyaml==5.4.1 (from ai-economist)\n",
      "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[54 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing lib3/PyYAML.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to lib3/PyYAML.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/justindiamond/opt/anaconda3/envs/torch/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 325, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 295, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 271, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 104, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 321, in run\n",
      "  \u001b[31m   \u001b[0m     self.find_sources()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 329, in find_sources\n",
      "  \u001b[31m   \u001b[0m     mm.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 550, in run\n",
      "  \u001b[31m   \u001b[0m     self.add_defaults()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 588, in add_defaults\n",
      "  \u001b[31m   \u001b[0m     sdist.add_defaults(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/command/sdist.py\", line 102, in add_defaults\n",
      "  \u001b[31m   \u001b[0m     super().add_defaults()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py\", line 250, in add_defaults\n",
      "  \u001b[31m   \u001b[0m     self._add_defaults_ext()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py\", line 335, in _add_defaults_ext\n",
      "  \u001b[31m   \u001b[0m     self.filelist.extend(build_ext.get_source_files())\n",
      "  \u001b[31m   \u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 201, in get_source_files\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/b0/r7xrgkbd7vv8jj4t_lq1t0jw0000gn/T/pip-build-env-jvt2wjuw/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n",
      "  \u001b[31m   \u001b[0m     raise AttributeError(attr)\n",
      "  \u001b[31m   \u001b[0m AttributeError: cython_sources\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "import os, signal, sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ai_economist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import foundation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mai_economist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m foundation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ai_economist'"
     ]
    }
   ],
   "source": [
    "# Import foundation\n",
    "from ai_economist import foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the tutorials folder\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\"/content/ai-economist/tutorials\")\n",
    "else:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from utils import plotting  # plotting utilities for visualizing env. state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Decision Process\n",
    "\n",
    "Formally, our economic simulation is a key part of a Markov Decision Process (MDP).\n",
    "\n",
    "MDPs describe episodes in which agents interact with a stateful environment in a continuous feedback loop. At each timestep, agents receive an observation and use a policy to choose actions. The environment then advances to a new state, using the old state and the chosen actions. The agents then receive new observations and rewards. This process repeats over $T$ timesteps (possibly infinite).\n",
    "\n",
    "The goal of each agent is to maximize its expected sum of future (discounted) rewards, by finding its optimal policy. Intuitively, this means that an agent needs to understand which (sequence of) actions lead to high rewards (in expectation).\n",
    "\n",
    "### References\n",
    "\n",
    "For more information on reinforcement learning and MDPs, check out:\n",
    "\n",
    "- Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. [http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a Simulation Environment (a Scenario Instance)\n",
    "\n",
    "The Scenario class implements an economic simulation with multiple agents and (optionally) a social planner. \n",
    "\n",
    "Scenarios provide a high-level gym-style API that lets agents interact with it. The API supports multi-agent observations, actions, etc.\n",
    "\n",
    "Each Scenario is stateful and implements two main methods:\n",
    "\n",
    "- __step__, which advances the simulation to the next state, and \n",
    "- __reset__, which puts the simulation back in an initial state.\n",
    "\n",
    "Each Scenario is customizable: you can specify options in a dictionary. Here is an example for a scenario with 4 agents:\n",
    "\n",
    "**Note: This config dictionary will likely seem fairly incomprehensible at this point in the tutorials. Don't worry. The advanced tutorial offers much more context. This is just to get things started and to provide a reference for how to create a \"free market\" economy from the AI Economist.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration of the environment that will be built\n",
    "\n",
    "env_config = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'layout_from_file/simple_wood_and_stone',\n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) Building houses\n",
    "        ('Build', {'skill_dist': \"pareto\", 'payment_max_skill_multiplier': 3}),\n",
    "        # (2) Trading collectible resources\n",
    "        ('ContinuousDoubleAuction', {'max_num_orders': 5}),\n",
    "        # (3) Movement and resource collection\n",
    "        ('Gather', {}),\n",
    "    ],\n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    'env_layout_file': 'quadrant_25x25_20each_30clump.txt',\n",
    "    'starting_agent_coin': 10,\n",
    "    'fixed_four_skill_and_loc': True,\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 4,          # Number of non-planner agents (must be > 1)\n",
    "    'world_size': [25, 25], # [Height, Width] of the env world\n",
    "    'episode_length': 1000, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': False,\n",
    "    'multi_action_mode_planner': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': False,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment instance using this configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = foundation.make_env_instance(**env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interacting with the Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "The Agent class holds the state of agents in the simulation. Each Agent instance represents a _logical_ agent.\n",
    "\n",
    "_Note that this might be separate from a Policy model that lives outside the Scenario and controls the Agent's behavior._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_agent(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A random policy\n",
    "\n",
    "Now let's interact with the simulation.\n",
    "\n",
    "Each Agent needs to choose which actions to execute using a __policy__.\n",
    "\n",
    "Agents might not always be allowed to execute all actions. For instance, a mobile Agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\". This information is given by a mask, which is provided under ```obs[<agent_id_str>][\"action_mask\"]``` in the observation dictionary ```obs``` returned by the scenario.\n",
    "\n",
    "Let's use a random policy to step through the simulation. The methods below implement a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The code for sampling actions (this cell), and playing an episode (below) are general.  \n",
    "# That is, it doesn't depend on the Scenario and Component classes used in the environment!\n",
    "\n",
    "def sample_random_action(agent, mask):\n",
    "    \"\"\"Sample random UNMASKED action(s) for agent.\"\"\"\n",
    "    # Return a list of actions: 1 for each action subspace\n",
    "    if agent.multi_action_mode:\n",
    "        split_masks = np.split(mask, agent.action_spaces.cumsum()[:-1])\n",
    "        return [np.random.choice(np.arange(len(m_)), p=m_/m_.sum()) for m_ in split_masks]\n",
    "\n",
    "    # Return a single action\n",
    "    else:\n",
    "        return np.random.choice(np.arange(agent.action_spaces), p=mask/mask.sum())\n",
    "\n",
    "def sample_random_actions(env, obs):\n",
    "    \"\"\"Samples random UNMASKED actions for each agent in obs.\"\"\"\n",
    "        \n",
    "    actions = {\n",
    "        a_idx: sample_random_action(env.get_agent(a_idx), a_obs['action_mask'])\n",
    "        for a_idx, a_obs in obs.items()\n",
    "    }\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to interact with the simulation...\n",
    "\n",
    "First, environments can be put in an initial state by using __reset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call __step__ to advance the state and advance time by one tick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = sample_random_actions(env, obs)\n",
    "obs, rew, done, info = env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the __step__ method composes several Components (which act almost like modular sub-Environments) that implement various agent affordances and environment dynamics. For more detailed information on Components and how to implement custom Component classes, see the advanced tutorial. For this tutorial, we will continue to inspect the information that __step__ returns and run a full episode in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Each observation is a dictionary that contains information for the $N$ agents and (optionally) social planner (with id \"p\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each agent, the agent-specific observation is a dictionary. Each Component can contribute information to the agent-specific observation. For instance, the Build Component contributes the \n",
    "\n",
    "- Build-build_payment (float)\n",
    "- Build-build_skill (int)\n",
    "\n",
    "fields, which are defined in the ```generate_observations``` method in [foundation/components/build.py](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/components/build.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, val in obs['0'].items(): \n",
    "    print(\"{:50} {}\".format(key, type(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "For each agent / planner, the reward dictionary contains a scalar reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_idx, reward in rew.items(): \n",
    "    print(\"{:2} {:.3f}\".format(agent_idx, reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "\n",
    "The __done__ object is a dictionary that by default records whether all agents have seen the end of the episode. The default criterion for each agent is to 'stop' their episode once $H$ steps have been executed. Once an agent is 'done', they do not change their state anymore. So, while it's not currently implemented, this could be used to indicate that the episode has ended *for a specific Agent*.\n",
    "\n",
    "In general, this is useful for telling a Reinforcement Learning framework when to reset the environment and how to organize the trajectories of individual Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info\n",
    "\n",
    "The __info__ object can record any auxiliary information from the simulator, which can be useful, e.g., for visualization. By default, this is empty. To modify this behavior, modify the step() method in [foundation/base/base_env.py](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/base/base_env.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sampling and Visualizing an Episode\n",
    "\n",
    "Let's step multiple times with this random policy and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plot(env, ax, fig):\n",
    "    \"\"\"Plots world state during episode sampling.\"\"\"\n",
    "    plotting.plot_env_state(env, ax)\n",
    "    ax.set_aspect('equal')\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def play_random_episode(env, plot_every=100, do_dense_logging=False):\n",
    "    \"\"\"Plays an episode with randomly sampled actions.\n",
    "    \n",
    "    Demonstrates gym-style API:\n",
    "        obs                  <-- env.reset(...)         # Reset\n",
    "        obs, rew, done, info <-- env.step(actions, ...) # Interaction loop\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    # Reset\n",
    "    obs = env.reset(force_dense_logging=do_dense_logging)\n",
    "\n",
    "    # Interaction loop (w/ plotting)\n",
    "    for t in range(env.episode_length):\n",
    "        actions = sample_random_actions(env, obs)\n",
    "        obs, rew, done, info = env.step(actions)\n",
    "\n",
    "        if ((t+1) % plot_every) == 0:\n",
    "            do_plot(env, ax, fig)\n",
    "\n",
    "    if ((t+1) % plot_every) != 0:\n",
    "        do_plot(env, ax, fig)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_episode(env, plot_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see four agents (indicated by a circled __\\*__) that move around in the 2-dimensional world. Light brown cells contain Stone, green cells contain Wood. Each agent can build Houses, indicated by corresponding colored cells. Water tiles (blue squares), which prevent movement, divide the map into four quadrants.\n",
    "\n",
    "Note: this is showing the same information as the image at the top of the tutorial -- it just uses a much more simplistic rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize using dense logging\n",
    "\n",
    "Environments built with Foundation provide a couple tools for logging. Perhaps the most useful are **dense logs**. When you reset the environment, you can tell it to create a dense log for the new episode. This will store Agent states at each point in time along with any Component-specific dense log information (say, about builds, trades, etc.) that the Components provide. In addition, it will periodically store a snapshot of the world state.\n",
    "\n",
    "We provide a few plotting tools that work well with the type of environment showcased here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play another episode. This time, tell the environment to do dense logging\n",
    "play_random_episode(env, plot_every=100, do_dense_logging=True)\n",
    "\n",
    "# Grab the dense log from the env\n",
    "dense_log = env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the world state from t=0 to t=200\n",
    "fig = plotting.vis_world_range(dense_log, t0=0, tN=200, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the world state over the full episode\n",
    "fig = plotting.vis_world_range(dense_log, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"breakdown\" tool to visualize the world state, agent-wise quantities, movement, and trading events\n",
    "plotting.breakdown(dense_log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that you've seen how to interact with the simulation and generate episodes, try the next tutorial ([economic_simulation_advanced.ipynb](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)) that explains how the simulation is composed of low-level Components and Entities. This structure enables flexible extensions of the economic simulation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948099c6ab02a15a055545cbca87e716aee9b3e6a51d0f68b03005577a92a5b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ai-economist': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
